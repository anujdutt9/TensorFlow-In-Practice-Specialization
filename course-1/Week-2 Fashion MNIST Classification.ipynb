{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Image from Dataset\n",
    "n = np.random.randint(0, len(X_train))\n",
    "img = X_train[n]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff42089e668>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label for the Sample Image\n",
    "y_train[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why use a number as a label instead of a word in a language ?***\n",
    "\n",
    "Using a number is a first step in avoiding bias -- instead of labelling it with words in a specific language and excluding people who don’t speak that language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes:  10\n"
     ]
    }
   ],
   "source": [
    "# Total number of classes\n",
    "unique_labels = np.unique(y_train)\n",
    "n_classes = len(unique_labels)\n",
    "print('Number of Classes: ', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Sumamry\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***When to use Categorical Cross Entropy vs Sparse Categorical Cross Entropy Loss Function ?***\n",
    "\n",
    "Both, **categorical cross entropy** and **sparse categorical cross entropy** have the same loss function. The only difference is the format in which you mention 𝑌𝑖 (i,e true labels).\n",
    "\n",
    "If your 𝑌𝑖's are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\n",
    "\n",
    "But if your 𝑌𝑖's are integers, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [1] , [2], [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 866us/step - loss: 0.4982 - val_loss: 0.4184\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 802us/step - loss: 0.3786 - val_loss: 0.3836\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 759us/step - loss: 0.3373 - val_loss: 0.3667\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.3130 - val_loss: 0.3521\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2965 - val_loss: 0.3610\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 866us/step - loss: 0.2800 - val_loss: 0.3439\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 846us/step - loss: 0.2701 - val_loss: 0.3391\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 808us/step - loss: 0.2575 - val_loss: 0.3261\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.2472 - val_loss: 0.3384\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2416 - val_loss: 0.3425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff471dd05f8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 538us/step - loss: 0.3425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3424900472164154"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Trained Model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-1\n",
    "\n",
    "***What does the following list represent ?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4753834e-07, 6.0551714e-10, 1.2445364e-10, ..., 1.4719782e-02,\n",
       "        1.5543051e-06, 9.7918463e-01],\n",
       "       [3.2161483e-05, 2.5979011e-10, 9.9776030e-01, ..., 1.6603248e-13,\n",
       "        1.1923962e-08, 1.1945603e-13],\n",
       "       [1.0618604e-07, 9.9999988e-01, 1.6887051e-11, ..., 2.1480732e-24,\n",
       "        6.3755711e-12, 1.8906526e-20],\n",
       "       ...,\n",
       "       [9.9394158e-05, 4.3073258e-12, 6.7607004e-07, ..., 5.7970090e-10,\n",
       "        9.9969876e-01, 1.4815297e-13],\n",
       "       [9.2866287e-08, 9.9995911e-01, 3.4771874e-09, ..., 6.4434796e-15,\n",
       "        2.4853455e-10, 3.8115896e-11],\n",
       "       [3.5189621e-06, 2.2403675e-07, 1.8330420e-06, ..., 2.4691424e-03,\n",
       "        7.3754003e-05, 2.2126538e-05]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Inference\n",
    "classifications = model.predict(X_test)\n",
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4753834e-07, 6.0551714e-10, 1.2445364e-10, 1.4571497e-09,\n",
       "       6.2587496e-10, 6.0936464e-03, 2.4535916e-07, 1.4719782e-02,\n",
       "       1.5543051e-06, 9.7918463e-01], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the first list of numbers\n",
    "classifications[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list defines the probability w.r.t each label i.e. the probability that the model thinks the given input image corresponds to label '0' is 2.4753834e-07.\n",
    "\n",
    "To get the predicted label, we take the index of the maximum probability. That tells us the corresponding predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the Labels, get the index with max probability value\n",
    "predicted_labels = np.argmax(classifications, axis=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-2\n",
    "\n",
    "***What's the impact of increasing the number of neurons from 128 to 1024 ?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4718 - val_loss: 0.4420\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3551 - val_loss: 0.3699\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3217 - val_loss: 0.3819\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2955 - val_loss: 0.3673\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2782 - val_loss: 0.3523\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2624 - val_loss: 0.3312\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2507 - val_loss: 0.3333\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2381 - val_loss: 0.3576\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2278 - val_loss: 0.3268\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2197 - val_loss: 0.3430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff471d23fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=1024, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of neurons in the dense layer from 128 to 1024 increased the training time and learning capacity of the model. Hence, the model accuracy increases slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-3\n",
    "\n",
    "***What would happen if you remove the Flatten() layer. Why do you think that's the case?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1567 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4783 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4176 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4091 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a24ed6aaa263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m model.fit(x=X_train, y=y_train, epochs=10,\n\u001b[1;32m     14\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           workers=4, use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1567 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4783 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4176 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/anujdutt/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4091 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).\n"
     ]
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    #tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the Flatten layer results in an error as follows: **ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).**\n",
    "\n",
    "This happens because the shape of the input is no longer same as the shape of the data. The input data has a shape of (28,28) whereas the model without the flatten layer is expecting the input data to be of shape (128,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-4\n",
    "\n",
    "***Consider the final (output) layers. Why are there 10 of them? What would happen if you had a different amount than 10? For example, try training the network with 5.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 6 2 8 9 0 1 3 8 6 0 5 5 5 0 8 6 2 0 4 6 5 0 6 1 5 4 2 5 5 5 5 7\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-21-c35aca51d0ea>:15) ]] [Op:__inference_train_function_46667]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c35aca51d0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m model.fit(x=X_train, y=y_train, epochs=10,\n\u001b[1;32m     14\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           workers=4, use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 6 2 8 9 0 1 3 8 6 0 5 5 5 0 8 6 2 0 4 6 5 0 6 1 5 4 2 5 5 5 5 7\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-21-c35aca51d0ea>:15) ]] [Op:__inference_train_function_46667]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=5, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will fail as the number of labels in the dataset is in the range of 0 to 9 whereas the ouput of the model is from 0 to 5. Hence, the model is unable to train.\n",
    "\n",
    "For the model to train, the number of output units need to be same as the number of classes in the label data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-5\n",
    "\n",
    "***Consider the effects of additional layers in the network. What will happen if you add another layer between the one with 512 and the final layer with 10.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4676 - val_loss: 0.4300\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3545 - val_loss: 0.3841\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3225 - val_loss: 0.3492\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2945 - val_loss: 0.3507\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2775 - val_loss: 0.3449\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2619 - val_loss: 0.3476\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2514 - val_loss: 0.3227\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2391 - val_loss: 0.3492\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2273 - val_loss: 0.3478\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2206 - val_loss: 0.3357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4614135f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a significant impact -- because this is relatively simple data. For far more complex data (including color images to be classified as flowers that you'll see in the next lesson), extra layers are often necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-6\n",
    "\n",
    "***Consider the impact of training for more or less epochs. Why do you think that would be the case?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.4978 - val_loss: 0.4235\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 694us/step - loss: 0.3778 - val_loss: 0.4060\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 756us/step - loss: 0.3403 - val_loss: 0.3754\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 727us/step - loss: 0.3134 - val_loss: 0.3535\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 720us/step - loss: 0.2965 - val_loss: 0.3580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4321ff6a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model with less epochs\n",
    "model.fit(x=X_train, y=y_train, epochs=5,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 1s 760us/step - loss: 0.4975 - val_loss: 0.4165\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 1s 710us/step - loss: 0.3752 - val_loss: 0.3816\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 1s 714us/step - loss: 0.3398 - val_loss: 0.3770\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 1s 713us/step - loss: 0.3136 - val_loss: 0.3853\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 1s 715us/step - loss: 0.2958 - val_loss: 0.3433\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 1s 722us/step - loss: 0.2812 - val_loss: 0.3735\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 1s 742us/step - loss: 0.2674 - val_loss: 0.3691\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 1s 741us/step - loss: 0.2584 - val_loss: 0.3494\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2491 - val_loss: 0.3612\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 2s 854us/step - loss: 0.2399 - val_loss: 0.3307\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.2336 - val_loss: 0.3363\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 1s 729us/step - loss: 0.2246 - val_loss: 0.3336\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 1s 688us/step - loss: 0.2187 - val_loss: 0.3503\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 1s 702us/step - loss: 0.2131 - val_loss: 0.3423\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 1s 695us/step - loss: 0.2054 - val_loss: 0.3434\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 1s 689us/step - loss: 0.2010 - val_loss: 0.3539\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 1s 691us/step - loss: 0.1958 - val_loss: 0.3527\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 1s 720us/step - loss: 0.1898 - val_loss: 0.3639\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 2s 863us/step - loss: 0.1865 - val_loss: 0.3484\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 2s 801us/step - loss: 0.1809 - val_loss: 0.3655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff472915748>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model with more epochs\n",
    "model.fit(x=X_train, y=y_train, epochs=20,\n",
    "          validation_data=(X_test, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-7\n",
    "\n",
    "***Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 746us/step - loss: 4.0755 - val_loss: 0.8333\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 708us/step - loss: 0.6834 - val_loss: 0.6747\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 710us/step - loss: 0.6044 - val_loss: 0.6563\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 718us/step - loss: 0.5697 - val_loss: 0.5902\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.5243 - val_loss: 0.5773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff461712860>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model with more epochs\n",
    "model.fit(x=X_train * 255.0, y=y_train, epochs=5,\n",
    "          validation_data=(X_test * 255.0, y_test), \n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to removal of normalization, the training and validation loss increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-8\n",
    "\n",
    "***Custom Callbacks***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4726 - val_loss: 0.4258\n",
      "Epoch 2/10\n",
      "1833/1875 [============================>.] - ETA: 0s - loss: 0.3599\n",
      "Reached 60% accuracy so cancelling training!\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3595 - val_loss: 0.3679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff472a27c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with the number of neurons in the layers. Inc to 1024\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Callback Class to stop training on reaching desired accuracy\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss')<0.4):\n",
    "            print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the Model with more epochs\n",
    "model.fit(x=X_train, y=y_train, epochs=10,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[callbacks],\n",
    "          workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a callback function, we are able to stop the training of the model above when the accruacy reaches 60%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

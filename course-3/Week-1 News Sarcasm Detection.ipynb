{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-16 12:09:50--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4006:806::2010, 2607:f8b0:4006:807::2010, 2607:f8b0:4006:81b::2010, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4006:806::2010|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: ‘./sarcasm.json’\n",
      "\n",
      "./sarcasm.json      100%[===================>]   5.38M  12.4MB/s    in 0.4s    \n",
      "\n",
      "2020-08-16 12:09:50 (12.4 MB/s) - ‘./sarcasm.json’ saved [5643545/5643545]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
    "    -O ./sarcasm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "with open(\"./sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
       " 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " 'is_sarcastic': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the Label, Sentence and the URL\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Textt\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29657"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary: Number of Unique Words with Index\n",
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "[308, 15115, 679, 3337, 2298, 48, 382, 2576, 15116, 6, 2577, 8434]\n"
     ]
    }
   ],
   "source": [
    "# Text to Token Mapping\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sentences[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Padding the Text\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(sentences[0])\n",
    "print(padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26709, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of Encoded Dataset\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that we have 26709 sentences, represented using text encodings, having a length of 40 each. The length is decided by the length of the longest sentence. That length is then set to the length for all the sentences and the missing values for shorter sentences are filled with 0's."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
